# Machine-Learning
Topics and tasks for the classical machine learning

1. Linear regression
2. The validation parameters
3. Simple classifiers and metrics
4. Multiclass classifier
5. Logistic regression
6. Border classifier
7. Decision Tree
8. Random Forest
9. AdaBoost
10. K-means

## Linear Regression
Train a simple regression model using the Moore-Penrose matrix. Polynomial regression.</br>
![](/LinearRegression/dataset.png)
## The validation parameters
Validation of hyperparameters. Regularization. Overfitting and Underfitting.</br>
![](/RegressionValidation/training.png)
## Simple classifiers and metrics
Simple classifier of football and basketball players.</br>
Creating a custom dataset using normal distribution.</br>
Metrics: TP, TN, FP, FN, Error Alpha, Error Beta, Accuracy, Precision, Recall, F1-score, ROC, PRC, AUC
## Multiclass classifier
Dataset digits sklearn. Classification of 10 digits. </br>
Classifier - the scalar product of the masked figures.</br>
![](/MulticlassClassifier/digits.png)
## Logistic regression
Logistic regression model. Softmax. Gradient descent. Initialization Xavier, He.</br>
![](/LogisticRegression/GradientDescent.png)</br>
![](/LogisticRegression/train.gif)
## Border classifier
Binary classification. Sigmoid. Building a border that divides the space.</br>
![](/BorderClassifier/classifier.png)
## Decision Tree
Binary decision tree. Root node, internal node (weak classifier), terminal node.</br>
Split function(hyperplanes parallel to the coordinate axes). Feature selection function.</br>
Criterions - Entropy (Information Gain), Gini, Error.</br>
Criterions for stopping tree growth - max deep, min sample, min criterion.</br>
![](/DecisionTree/ExampleTree.png)
## Random Forest
Ensemble of classifiers. Different trees. Bagging. Random Node Optimization.
![](/RandomForest/ExampleRandomForest.png)
## AdaBoost
TODO
## K-means
TODO
